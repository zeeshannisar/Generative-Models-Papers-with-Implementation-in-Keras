{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN-Implementation-MNIST.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPTW6rCqSd898uSa7JRlkZT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"blWx3onW3X-P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"64dae4b6-cb7a-438d-8c3c-b109db7c2e1b","executionInfo":{"status":"ok","timestamp":1582694136583,"user_tz":-300,"elapsed":22651,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J7RkOlGy35bD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"d9cf4e01-e5c0-4e24-8d7d-65f1c5747c92","executionInfo":{"status":"ok","timestamp":1582694137740,"user_tz":-300,"elapsed":4679,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["import tensorflow as tf\n","tf.VERSION"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'1.15.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"tprXrXhs5jy7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"fb016f35-9fb8-48a0-d30a-ea998dc0f24b","executionInfo":{"status":"ok","timestamp":1582694144909,"user_tz":-300,"elapsed":10291,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["#To import InstanceNormalization from keras-contrib\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-6doia83r\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-6doia83r\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.5)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=0feed905098a1f92f02d5b11c2257d93cc6874f1b70077507657299e5ec5241b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-k3ru5cj4/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_G1W0NTj59Iv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"c118b3ed-b10e-46d7-a4bc-c0dc28f3920d","executionInfo":{"status":"ok","timestamp":1582694211103,"user_tz":-300,"elapsed":872,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["from keras.datasets import mnist\n","import numpy as np\n","\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","condition_3 = np.where((Y_train == 3))\n","imgs_3= X_train[condition_3]\n","\n","condition_6 = np.where((Y_train == 6))\n","imgs_6 = X_train[condition_6]\n","\n","\n","print('imgs-3 : {}'.format(imgs_3.shape))\n","print('imgs-6 : {}'.format(imgs_6.shape))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["imgs-3 : (6131, 28, 28)\n","imgs-6 : (5918, 28, 28)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Crh5dnnM6yl1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"11c42005-9648-47ef-9c9e-5086094b81e4","executionInfo":{"status":"ok","timestamp":1582694509103,"user_tz":-300,"elapsed":1829,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["import cv2\n","import numpy as np\n","Train_3=[]\n","Train_6=[]\n","\n","for img in imgs_3[:]:\n","  resize=cv2.resize(img, (32,32), fx=0.5, fy=0.5, interpolation = cv2.INTER_CUBIC)\n","  normalize=resize/255.0\n","  Train_3.append(normalize)\n","  \n","\n","  \n","for img in imgs_6[:]:\n","  resize=cv2.resize(img, (32,32), fx=0.5, fy=0.5, interpolation = cv2.INTER_CUBIC)\n","  normalize=resize/255.0\n","  Train_6.append(normalize)\n","  \n","\n","\n","Train_3=np.array(Train_3).reshape(-1,32,32,1)  \n","Train_6=np.array(Train_6).reshape(-1,32,32,1)\n","\n","print('Train-3 : {}'.format(Train_3.shape))\n","print('Train-6 : {}'.format(Train_6.shape))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Train-3 : (6131, 32, 32, 1)\n","Train-6 : (5918, 32, 32, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CE0BPSvk36qi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8975e7a2-8e70-42c7-dd69-28fcbc3e89d1","executionInfo":{"status":"ok","timestamp":1582699473770,"user_tz":-300,"elapsed":1769,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","\n","import sys\n","import numpy as np\n","import os\n","\n","# Input shape\n","img_rows = 32\n","img_cols = 32\n","channels = 1\n","img_shape = (img_rows, img_cols, channels)\n","\n","# Calculate output shape of D (PatchGAN)\n","patch = int(img_rows/2**3)\n","disc_patch = (patch, patch, 1)\n","\n","# Loss weights\n","lambda_cycle = 10.0               # Cycle-consistency loss\n","lambda_id = 0.1 * lambda_cycle    # Identity loss\n","\n","#Optimizer\n","optimizer = Adam(0.0002, 0.5)\n","\n","def Generator():\n","  \"\"\"Generator foloows U-Net Architecture\"\"\"\n","\n","  # Image input\n","  inputs = Input(shape=img_shape)\n","\n","  # Downsampling\n","  conv1 = Conv2D(filters=32, kernel_size=4, strides=2, padding='same')(inputs)\n","  conv1 = LeakyReLU(alpha=0.2)(conv1)\n","  conv1 = InstanceNormalization()(conv1)\n","\n","  conv2 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(conv1)\n","  conv2 = LeakyReLU(alpha=0.2)(conv2)\n","  conv2 = InstanceNormalization()(conv2)\n","\n","  conv3 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(conv2)\n","  conv3 = LeakyReLU(alpha=0.2)(conv3)\n","  conv3 = InstanceNormalization()(conv3)\n","  \n","  # Upsampling\n","  deconv1 = UpSampling2D(size=2)(conv3)\n","  deconv1 = Conv2D(filters=128, kernel_size=4, strides=1, padding='same', activation='relu')(deconv1)\n","  deconv1 = InstanceNormalization()(deconv1)\n","  deconv1 = Concatenate()([deconv1, conv2])\n","\n","  deconv2 = UpSampling2D(size=2)(deconv1)\n","  deconv2 = Conv2D(filters=64, kernel_size=4, strides=1, padding='same', activation='relu')(deconv2)\n","  deconv2 = InstanceNormalization()(deconv2)\n","  deconv2 = Concatenate()([deconv2, conv1])\n","\n","  deconv3 = UpSampling2D(size=2)(deconv2)\n","  outputs = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(deconv3)\n","\n","  return Model(inputs = inputs, outputs = outputs)\n","\n","\n","def Discriminator():\n","  inputs = Input(shape=img_shape)\n","  \n","  outputs = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(inputs)\n","  outputs = LeakyReLU(alpha=0.2)(outputs)\n","\n","  outputs = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(outputs)\n","  outputs = LeakyReLU(alpha=0.2)(outputs)\n","  outputs = InstanceNormalization()(outputs)\n","\n","  outputs = Conv2D(filters=256, kernel_size=4, strides=2, padding='same')(outputs)\n","  outputs = LeakyReLU(alpha=0.2)(outputs)\n","  outputs = InstanceNormalization()(outputs)\n","  \n","  outputs = Conv2D(1, kernel_size=4, strides=1, padding='same')(outputs)\n","\n","  return Model(inputs = inputs, outputs = outputs)\n","\n","# Build and compile the discriminators\n","D_A = Discriminator()\n","D_A.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n","print(D_A.summary())\n","\n","D_B = Discriminator()\n","D_B.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n","\n","#-------------------------\n","# Construct Computational\n","#   Graph of Generators\n","#-------------------------\n","\n","# Build the generators\n","G_AB = Generator()\n","print(G_AB.summary())\n","G_BA = Generator()\n","\n","# Input images from both domains\n","img_A = Input(shape=img_shape)\n","img_B = Input(shape=img_shape)\n","\n","# Translate images to the other domain\n","fake_B = G_AB(img_A)\n","fake_A = G_BA(img_B)\n","# Translate images back to original domain\n","reconstr_A = G_BA(fake_B)\n","reconstr_B = G_AB(fake_A)\n","# Identity mapping of images\n","img_A_id = G_BA(img_A)\n","img_B_id = G_AB(img_B)\n","\n","# For the combined model we will only train the generators\n","D_A.trainable = False\n","D_B.trainable = False\n","\n","# Discriminators determines validity of translated images\n","valid_A = D_A(fake_A)\n","valid_B = D_B(fake_B)\n","\n","# Combined model trains generators to fool discriminators\n","combined = Model(inputs=[img_A, img_B], outputs=[valid_A, valid_B, reconstr_A, reconstr_B, img_A_id, img_B_id ])\n","combined.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights=[1, 1, lambda_cycle, lambda_cycle, lambda_id, lambda_id], optimizer=optimizer)\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Model: \"model_11\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_13 (InputLayer)        (None, 32, 32, 1)         0         \n","_________________________________________________________________\n","conv2d_43 (Conv2D)           (None, 16, 16, 64)        1088      \n","_________________________________________________________________\n","leaky_re_lu_27 (LeakyReLU)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_44 (Conv2D)           (None, 8, 8, 128)         131200    \n","_________________________________________________________________\n","leaky_re_lu_28 (LeakyReLU)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","instance_normalization_31 (I (None, 8, 8, 128)         2         \n","_________________________________________________________________\n","conv2d_45 (Conv2D)           (None, 4, 4, 256)         524544    \n","_________________________________________________________________\n","leaky_re_lu_29 (LeakyReLU)   (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","instance_normalization_32 (I (None, 4, 4, 256)         2         \n","_________________________________________________________________\n","conv2d_46 (Conv2D)           (None, 4, 4, 1)           4097      \n","=================================================================\n","Total params: 660,933\n","Trainable params: 660,933\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Model: \"model_13\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_15 (InputLayer)           (None, 32, 32, 1)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 16, 16, 32)   544         input_15[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_33 (LeakyReLU)      (None, 16, 16, 32)   0           conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","instance_normalization_35 (Inst (None, 16, 16, 32)   2           leaky_re_lu_33[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 8, 8, 64)     32832       instance_normalization_35[0][0]  \n","__________________________________________________________________________________________________\n","leaky_re_lu_34 (LeakyReLU)      (None, 8, 8, 64)     0           conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","instance_normalization_36 (Inst (None, 8, 8, 64)     2           leaky_re_lu_34[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 4, 4, 128)    131200      instance_normalization_36[0][0]  \n","__________________________________________________________________________________________________\n","leaky_re_lu_35 (LeakyReLU)      (None, 4, 4, 128)    0           conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","instance_normalization_37 (Inst (None, 4, 4, 128)    2           leaky_re_lu_35[0][0]             \n","__________________________________________________________________________________________________\n","up_sampling2d_13 (UpSampling2D) (None, 8, 8, 128)    0           instance_normalization_37[0][0]  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 8, 8, 128)    262272      up_sampling2d_13[0][0]           \n","__________________________________________________________________________________________________\n","instance_normalization_38 (Inst (None, 8, 8, 128)    2           conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 8, 8, 192)    0           instance_normalization_38[0][0]  \n","                                                                 instance_normalization_36[0][0]  \n","__________________________________________________________________________________________________\n","up_sampling2d_14 (UpSampling2D) (None, 16, 16, 192)  0           concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 16, 16, 64)   196672      up_sampling2d_14[0][0]           \n","__________________________________________________________________________________________________\n","instance_normalization_39 (Inst (None, 16, 16, 64)   2           conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 16, 16, 96)   0           instance_normalization_39[0][0]  \n","                                                                 instance_normalization_35[0][0]  \n","__________________________________________________________________________________________________\n","up_sampling2d_15 (UpSampling2D) (None, 32, 32, 96)   0           concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 32, 32, 1)    1537        up_sampling2d_15[0][0]           \n","==================================================================================================\n","Total params: 625,067\n","Trainable params: 625,067\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vJT2_8C7ALB7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"14hYQrekRj0iqWtjepkqRWZX0BCra2EEL"},"outputId":"430741a4-5904-457d-a168-01bc20f488e7","executionInfo":{"status":"ok","timestamp":1582702372705,"user_tz":-300,"elapsed":2796178,"user":{"displayName":"ZEESHAN NISAR","photoUrl":"","userId":"05353617243036990298"}}},"source":["import datetime\n","import sys\n","import matplotlib.pyplot as plt\n","import random\n","\n","os.chdir('/content/drive/My Drive/GitHub Repositories/Generative Models Papers with Implementation in Keras')\n","baseDir = './CycleGAN-Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks'\n","outputDir = os.path.join(baseDir, 'MNIST outputs')\n","if not os.path.exists(outputDir):\n","    os.makedirs(outputDir)\n","    print('Output Directory Created to save Results')\n","\n","start_time = datetime.datetime.now()\n","epochs = 5\n","batch_size = 1\n","batches=int(5500/batch_size)\n","save_interval = 200\n","#limitize the training data to first 5500 images and use rest for test data\n","limit = 5500\n","# Adversarial loss ground truths\n","valid = np.ones((batch_size,) + disc_patch)\n","fake = np.zeros((batch_size,) + disc_patch)\n","\n","\n","\n","\n","def save_imgs(epoch, batch):\n","  # pick random image from test data based on random indexes and reshape the image to tensor\n","  index_A = random.randint(limit, Train_3.shape[0])\n","  imgs_A = Train_3[index_A]\n","  imgs_A = imgs_A.reshape(-1, img_rows, img_cols, channels)\n","\n","  index_B = random.randint(limit, Train_6.shape[0])\n","  imgs_B = Train_6[index_B]\n","  imgs_B = imgs_B.reshape(-1, img_rows, img_cols, channels)\n","\n","  # Translate images to opposite domain\n","  fake_B = G_AB.predict(imgs_A)\n","  fake_A = G_BA.predict(imgs_B)  \n","  \n","  # Translate back to original domain\n","  reconstr_A = G_BA.predict(fake_B)\n","  reconstr_B = G_AB.predict(fake_A)\n","\n","  gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n","\n","  # Rescale images 0 - 1\n","  gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","  titles = ['Original-3', 'Translated-6', 'Reconstructed-3', 'Original-6', 'Translated-3', 'Reconstructed-6']\n","  fig=plt.figure(1)\n","  for i in range(6):\n","    plt.subplot(2, 3, i+1)\n","    plt.imshow(gen_imgs[i,..., 0], cmap='gray')\n","    plt.gca().set_title(titles[i])\n","    plt.axis('off')\n","  plt.show()\n","  fig.savefig(outputDir+'/image_at_epoch_{:04d}_batch_{:04d}.png'.format(epoch, batch))\n","  plt.close()\n","\n","# Training \n","for epoch in range(epochs):\n","  epoch+=1\n","  for batch in range(batches):\n","    batch+=1\n","    # ----------------------\n","    #  Train Discriminators\n","    # ----------------------\n","\n","    # pick random image from training data based on random indexes and reshape the image to tensor\n","    index_A = random.randint(0, limit)\n","    imgs_A = Train_3[index_A]\n","    imgs_A = imgs_A.reshape(-1, img_rows, img_cols, channels)\n","\n","    index_B = random.randint(0, limit)\n","    imgs_B = Train_6[index_B]\n","    imgs_B = imgs_B.reshape(-1, img_rows, img_cols, channels)\n","\n","\n","    # Translate images to opposite domain\n","    fake_B = G_AB.predict(imgs_A)\n","    fake_A = G_BA.predict(imgs_B)\n","    \n","    # Train the discriminators (original images = real / translated = Fake)\n","    dA_loss_real = D_A.train_on_batch(imgs_A, valid)\n","    dA_loss_fake = D_A.train_on_batch(fake_A, fake)\n","    dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n","\n","    dB_loss_real = D_B.train_on_batch(imgs_B, valid)\n","    dB_loss_fake = D_B.train_on_batch(fake_B, fake)\n","    dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n","\n","    # Total disciminator loss\n","    d_loss = 0.5 * np.add(dA_loss, dB_loss)\n","    \n","    # ------------------\n","    #  Train Generators\n","    # ------------------\n","\n","    # Train the generators\n","    g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, valid, imgs_A, imgs_B, imgs_A, imgs_B])\n","\n","    elapsed_time = datetime.datetime.now() - start_time\n","    sys.stdout.write('\\r Epoch: {0}/{1} | Batch: {2}/{3} | D-loss: {4:.3f}, D-acc: {5:.2f}% | G-loss: {6:.3f} | adv: {7:.3f} | recon: {8:.3f} | id: {9:.3f} | time: {10}'.\n","                     format(epoch, epochs, batch, batches, d_loss[0], 100*d_loss[1], g_loss[0], np.mean(g_loss[1:3]), np.mean(g_loss[3:5]),\n","                            np.mean(g_loss[5:6]), elapsed_time))\n","    # After specific number of iterations say 200 in our case save results\n","    # If at save interval => save generated image samples\n","    if batch % save_interval == 0:\n","      save_imgs(epoch, batch)"],"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"k4atUDxaPbWN","colab_type":"code","colab":{}},"source":["import imageio\n","import glob\n","images = []\n","for file_name in os.listdir(outputDir):\n","    if file_name.endswith('.png'):\n","        file_path = os.path.join(outputDir, file_name)\n","        images.append(imageio.imread(file_path))\n","imageio.mimsave(os.path.join(baseDir, 'CycleGAN-MNIST-training-outputs.gif'), images, fps = 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODUzJTJfTfx8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}