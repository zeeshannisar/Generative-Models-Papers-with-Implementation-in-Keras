{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32270,
     "status": "ok",
     "timestamp": 1582625236180,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "xa5ED42g3rJB",
    "outputId": "db55e31d-aedc-4852-c19e-a3e552053ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5200,
     "status": "ok",
     "timestamp": 1582625237636,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "oW_oa4hz5V97",
    "outputId": "770c6bcd-8e13-4739-9824-9da1c332e93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11794,
     "status": "ok",
     "timestamp": 1582625245793,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "O1dkxMxnC4c8",
    "outputId": "0878e950-721b-4686-a1e2-e1a8cc3a9d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to c:\\users\\zeeshan\\appdata\\local\\temp\\pip-req-build-4hdrv_x1\n",
      "Requirement already satisfied: keras in c:\\users\\zeeshan\\anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages (from keras-contrib==2.0.8) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\zeeshan\\appdata\\roaming\\python\\python37\\site-packages (from keras->keras-contrib==2.0.8) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\zeeshan\\appdata\\roaming\\python\\python37\\site-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\zeeshan\\appdata\\roaming\\python\\python37\\site-packages (from keras->keras-contrib==2.0.8) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\zeeshan\\anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages (from keras->keras-contrib==2.0.8) (5.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\zeeshan\\anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in c:\\users\\zeeshan\\anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in c:\\users\\zeeshan\\anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py): started\n",
      "  Building wheel for keras-contrib (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101659 sha256=e8d2ac2d4d507a7b7db489f64400b62ef718a08adbfba22fa79112dbc8bb7899\n",
      "  Stored in directory: C:\\Users\\Zeeshan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-q2apy75s\\wheels\\bb\\1f\\f2\\b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
      "Successfully built keras-contrib\n",
      "Installing collected packages: keras-contrib\n",
      "Successfully installed keras-contrib-2.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\Zeeshan\\AppData\\Local\\Temp\\pip-req-build-4hdrv_x1'\n"
     ]
    }
   ],
   "source": [
    "# To import InstanceNormalization we need to install keras_contrib\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19736,
     "status": "ok",
     "timestamp": 1582625256716,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "Ozns3f1d6VVi",
    "outputId": "1b529d90-3dea-4853-ea41-c423d90e7a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 6)  0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  131200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 128)  512         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 256)  524544      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 256)  1024        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 512)  2097664     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 512)  2048        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 1)    8193        batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,771,393\n",
      "Trainable params: 2,769,601\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\Zeeshan\\Anaconda3\\envs\\tensorflow_1.15\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 3136        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  131200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 256)  524544      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  2097664     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 512)  2048        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 512)    4194816     batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 512)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 512)    2048        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 512)    4194816     batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 4, 4, 512)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4, 4, 512)    2048        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 2, 2, 512)    4194816     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 2, 2, 512)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2, 2, 512)    2048        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 4, 4, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 512)    4194816     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 4, 1024)   0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 1024)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 512)    8389120     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 1024)   0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 1024) 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 512)  8389120     up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 1024) 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 256)  4194560     up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 512)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 512)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  1048704     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 256)  0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 256 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 262208      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 128 0           batch_normalization_15[0][0]     \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 128 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 3)  6147        up_sampling2d_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 41,843,331\n",
      "Trainable params: 41,834,499\n",
      "Non-trainable params: 8,832\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Input shape\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "#Optimizer\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "def Generator():\n",
    "  \"\"\"Generator foloows U-Net Architecture\"\"\"\n",
    "\n",
    "  # Image input\n",
    "  inputs = Input(shape=img_shape)\n",
    "\n",
    "  # Downsampling\n",
    "  conv1 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(inputs)\n",
    "  conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "\n",
    "  conv2 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(conv1)\n",
    "  conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "  conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "\n",
    "  conv3 = Conv2D(filters=256, kernel_size=4, strides=2, padding='same')(conv2)\n",
    "  conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "  conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "\n",
    "  conv4 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(conv3)\n",
    "  conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "  conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "\n",
    "  conv5 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(conv4)\n",
    "  conv5 = LeakyReLU(alpha=0.2)(conv5)\n",
    "  conv5 = BatchNormalization(momentum=0.8)(conv5)\n",
    "\n",
    "  conv6 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(conv5)\n",
    "  conv6 = LeakyReLU(alpha=0.2)(conv6)\n",
    "  conv6 = BatchNormalization(momentum=0.8)(conv6)\n",
    "\n",
    "  conv7 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(conv6)\n",
    "  conv7 = LeakyReLU(alpha=0.2)(conv7)\n",
    "  conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "  \n",
    "  # Upsampling\n",
    "  deconv1 = UpSampling2D(size=2)(conv7)\n",
    "  deconv1 = Conv2D(filters=512, kernel_size=4, strides=1, padding='same', activation='relu')(deconv1)\n",
    "  deconv1 = BatchNormalization(momentum=0.8)(deconv1)\n",
    "  deconv1 = Concatenate()([deconv1, conv6])\n",
    "\n",
    "  deconv2 = UpSampling2D(size=2)(deconv1)\n",
    "  deconv2 = Conv2D(filters=512, kernel_size=4, strides=1, padding='same', activation='relu')(deconv2)\n",
    "  deconv2 = BatchNormalization(momentum=0.8)(deconv2)\n",
    "  deconv2 = Concatenate()([deconv2, conv5])\n",
    "\n",
    "  deconv3 = UpSampling2D(size=2)(deconv2)\n",
    "  deconv3 = Conv2D(filters=512, kernel_size=4, strides=1, padding='same', activation='relu')(deconv3)\n",
    "  deconv3 = BatchNormalization(momentum=0.8)(deconv3)\n",
    "  deconv3 = Concatenate()([deconv3, conv4])\n",
    "\n",
    "  deconv4 = UpSampling2D(size=2)(deconv3)\n",
    "  deconv4 = Conv2D(filters=256, kernel_size=4, strides=1, padding='same', activation='relu')(deconv4)\n",
    "  deconv4 = BatchNormalization(momentum=0.8)(deconv4)\n",
    "  deconv4 = Concatenate()([deconv4, conv3])\n",
    "\n",
    "  deconv5 = UpSampling2D(size=2)(deconv4)\n",
    "  deconv5 = Conv2D(filters=128, kernel_size=4, strides=1, padding='same', activation='relu')(deconv5)\n",
    "  deconv5 = BatchNormalization(momentum=0.8)(deconv5)\n",
    "  deconv5 = Concatenate()([deconv5, conv2])\n",
    "\n",
    "  deconv6 = UpSampling2D(size=2)(deconv5)\n",
    "  deconv6 = Conv2D(filters=64, kernel_size=4, strides=1, padding='same', activation='relu')(deconv6)\n",
    "  deconv6 = BatchNormalization(momentum=0.8)(deconv6)\n",
    "  deconv6 = Concatenate()([deconv6, conv1])\n",
    "\n",
    "  deconv7 = UpSampling2D(size=2)(deconv6)\n",
    "  outputs = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(deconv7)\n",
    "\n",
    "  return Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "\n",
    "def Discriminator():\n",
    "  img_A = Input(shape=img_shape)\n",
    "  img_B = Input(shape=img_shape)\n",
    "\n",
    "  # Concatenate image and conditioning image by channels to produce input\n",
    "  combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "  outputs = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(combined_imgs)\n",
    "  outputs = LeakyReLU(alpha=0.2)(outputs)\n",
    "\n",
    "  outputs = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(outputs)\n",
    "  outputs = LeakyReLU(alpha=0.2)(outputs)\n",
    "  outputs = BatchNormalization(momentum=0.8)(outputs)\n",
    "\n",
    "  outputs = Conv2D(filters=256, kernel_size=4, strides=2, padding='same')(outputs)\n",
    "  outputs = LeakyReLU(alpha=0.2)(outputs)\n",
    "  outputs = BatchNormalization(momentum=0.8)(outputs)\n",
    "\n",
    "  outputs = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(outputs)\n",
    "  outputs = LeakyReLU(alpha=0.2)(outputs)\n",
    "  outputs = BatchNormalization(momentum=0.8)(outputs)\n",
    "  \n",
    "  outputs = Conv2D(1, kernel_size=4, strides=1, padding='same')(outputs)\n",
    "\n",
    "  return Model(inputs = [img_A, img_B], outputs = outputs)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "D = Discriminator()\n",
    "D.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(D.summary())\n",
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generator\n",
    "#-------------------------\n",
    "\n",
    "# Build the generator\n",
    "G = Generator()\n",
    "print(G.summary())\n",
    "\n",
    "# Input images and their conditioning images\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "fake_A = G(img_B)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "D.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid = D([fake_A, img_B])\n",
    "\n",
    "combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64887,
     "status": "error",
     "timestamp": 1582625321628,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "BSkjGQqvCIht",
    "outputId": "7f464ef0-906b-4a91-814c-a34525c0a0a4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Pix2PixGAN-Image-to-Image Translation with Conditional Adversarial Networks\\\\datasets\\\\edges2shoes\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1de4798aba9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbaseDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../Pix2PixGAN-Image-to-Image Translation with Conditional Adversarial Networks'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrainDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'datasets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'edges2shoes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrainImages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainImages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Pix2PixGAN-Image-to-Image Translation with Conditional Adversarial Networks\\\\datasets\\\\edges2shoes\\\\train'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 1\n",
    "resize = (256, 256)\n",
    "\n",
    "# os.chdir('/content/drive/My Drive/GitHub Repositories/Generative Models Papers with Implementation in Keras')\n",
    "baseDir = '../Pix2PixGAN-Image-to-Image Translation with Conditional Adversarial Networks'\n",
    "trainDir = os.path.join(baseDir, 'datasets', 'edges2shoes', 'train')\n",
    "trainImages = os.listdir(trainDir)\n",
    "batches = int(len(trainImages)/batch_size)\n",
    "\n",
    "validDir = os.path.join(baseDir, 'datasets', 'edges2shoes', 'valid')\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "outputDir = os.path.join(baseDir, 'outputs')\n",
    "if not os.path.exists(outputDir):\n",
    "  os.makedirs(outputDir)\n",
    "  print('Output Directory Created to save Results')\n",
    "  \n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones((batch_size,) + disc_patch)\n",
    "fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "  r, c = 3, 3\n",
    "  imgs_A = []\n",
    "  imgs_B = []\n",
    "  pickRandomImage = np.random.choice(os.listdir(validDir), size=batch_size)\n",
    "  img = Image.open(os.path.join(validDir, pickRandomImage))\n",
    "  h, w, _ = img.shape\n",
    "  _w = int(w/2)\n",
    "\n",
    "  img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "  img_A = img_A.resize(resize)\n",
    "  img_B = img_B.resize(resize)\n",
    "  imgs_A.append(img_A)\n",
    "  imgs_B.append(img_B)\n",
    "  imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "  imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "  \n",
    "  fake_A = G.predict(imgs_B)\n",
    "  gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "  # Rescale images 0 - 1\n",
    "  gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "  titles = ['Condition', 'Generated', 'Original']\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  cnt = 0\n",
    "  for i in range(r):\n",
    "      for j in range(c):\n",
    "          axs[i,j].imshow(gen_imgs[cnt])\n",
    "          axs[i, j].set_title(titles[i])\n",
    "          axs[i,j].axis('off')\n",
    "          cnt += 1\n",
    "  fig.savefig(outputDir+'/image_at_epoch_{:03d}.png'.format(epoch))\n",
    "  plt.close()\n",
    "        \n",
    "for epoch in range(epochs):\n",
    "  epoch+=1\n",
    "  for batch in batches:\n",
    "    batch+=1\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "    imgs_A = []\n",
    "    imgs_B = []\n",
    "    pickRandomImage = np.random.choice(trainImages, size=batch_size)\n",
    "    img = Image.open(os.path.join(trainDir, pickRandomImage))\n",
    "    print(img.shape)\n",
    "    h, w, _= img.shape\n",
    "    _w = int(w/2)\n",
    "\n",
    "    img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "    img_A = img_A.resize(resize)\n",
    "    img_B = img_B.resize(resize)\n",
    "    # If training => do random flip\n",
    "    if np.random.random() < 0.5:\n",
    "      img_A = np.fliplr(img_A)\n",
    "      img_B = np.fliplr(img_B)\n",
    "\n",
    "    imgs_A.append(img_A)\n",
    "    imgs_B.append(img_B)\n",
    "\n",
    "    imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "    imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "    # Condition on B and generate a translated version\n",
    "    fake_A = G.predict(imgs_B)\n",
    "    # Train the discriminators (original images = real / generated = Fake)\n",
    "    d_loss_real = D.train_on_batch([imgs_A, imgs_B], valid)\n",
    "    d_loss_fake = D.train_on_batch([fake_A, imgs_B], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    # -----------------\n",
    "    #  Train Generator\n",
    "    # -----------------\n",
    "\n",
    "    g_loss = combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    # Plot the progress\n",
    "    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs, batch, batches,\n",
    "                                                                                           d_loss[0], 100*d_loss[1], g_loss[0], elapsed_time))\n",
    "    \n",
    "  # After each epoch => save generated image samples\n",
    "  save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61964,
     "status": "error",
     "timestamp": 1582625472117,
     "user": {
      "displayName": "ZEESHAN NISAR",
      "photoUrl": "",
      "userId": "05353617243036990298"
     },
     "user_tz": -300
    },
    "id": "3URqi3YaLaln",
    "outputId": "8caf3c28-7401-4c68-f0f5-844f274f7a33"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8b2a0c507050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainImages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: './Pix2PixGAN-Image-to-Image Translation with Conditional Adversarial Networks/datasets/edges2shoes/train'"
     ]
    }
   ],
   "source": [
    "trainImages = os.listdir(trainDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRTwXfSBz4gx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNargTPGc3zsgAyKWmqZhHU",
   "collapsed_sections": [],
   "name": "Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
